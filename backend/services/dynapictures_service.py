#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Service de g√©n√©ration d'images avec Stable Diffusion pour Le Mixologue Augment√©

G√®re la g√©n√©ration d'images de cocktails en utilisant des mod√®les Stable Diffusion
avanc√©s (SDXL, SD 2.1, SD 1.5) avec authentification Hugging Face.
Supporte la s√©lection automatique du meilleur mod√®le selon les ressources disponibles.

Auteur: Assistant IA
Date: 2024
"""

import os
import logging
import torch
from typing import Dict, Optional, Any, Tuple
from datetime import datetime
from diffusers import (
    DiffusionPipeline, 
    StableDiffusionXLPipeline,
    StableDiffusionPipeline,
    EulerDiscreteScheduler
)
from huggingface_hub import login
from PIL import Image
import re

logger = logging.getLogger(__name__)

class DynaPicturesService:
    """
    Service de g√©n√©ration d'images avec Stable Diffusion avanc√©.
    
    Supporte plusieurs mod√®les Stable Diffusion (SDXL, SD 2.1, SD 1.5) avec
    s√©lection automatique du meilleur mod√®le selon les ressources disponibles.
    Optimise automatiquement les param√®tres selon le mod√®le choisi.
    """
    
    # Configuration des mod√®les disponibles
    MODELS_CONFIG = {
        "sdxl": {
            "id": "stabilityai/stable-diffusion-xl-base-1.0",
            "pipeline_class": StableDiffusionXLPipeline,
            "resolution": (1024, 1024),
            "min_vram_gb": 10,
            "steps": 25,
            "guidance_scale": 7.5,
            "description": "SDXL 1.0 - Qualit√© sup√©rieure, r√©solution 1024x1024"
        },
        "sd21": {
            "id": "stabilityai/stable-diffusion-2-1",
            "pipeline_class": StableDiffusionPipeline,
            "resolution": (768, 768),
            "min_vram_gb": 6,
            "steps": 20,
            "guidance_scale": 7.5,
            "description": "SD 2.1 - Bon √©quilibre qualit√©/performance, r√©solution 768x768"
        },
        "sd15": {
            "id": "runwayml/stable-diffusion-v1-5",
            "pipeline_class": StableDiffusionPipeline,
            "resolution": (512, 512),
            "min_vram_gb": 4,
            "steps": 15,
            "guidance_scale": 7.0,
            "description": "SD 1.5 - Rapide et compatible, r√©solution 512x512"
        }
    }
    
    def __init__(self, preferred_model: str = "auto"):
        """
        Initialise le service Stable Diffusion avec s√©lection automatique du mod√®le.
        
        Args:
            preferred_model: Mod√®le pr√©f√©r√© ("sdxl", "sd21", "sd15", "auto")
                           "auto" s√©lectionne automatiquement selon les ressources
        """
        self.hf_token = os.getenv('HUGGINGFACE_TOKEN')
        if not self.hf_token or self.hf_token == '<VOTRE-TOKEN-HUGGINGFACE>':
            raise ValueError(
                "HUGGINGFACE_TOKEN non configur√©. "
                "Veuillez d√©finir cette variable d'environnement avec votre token Hugging Face. "
                "G√©n√©rez un token sur https://huggingface.co/settings/tokens"
            )
        
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.current_model = None
        self.model_config = None
        
        # S√©lection du mod√®le optimal
        self.selected_model = self._select_optimal_model(preferred_model)
        self.model_config = self.MODELS_CONFIG[self.selected_model]
        logger.info(f"üéØ Mod√®le s√©lectionn√©: {self.selected_model} - {self.model_config['description']}")
        
        # Authentification Hugging Face
        try:
            login(token=self.hf_token)
            logger.info("‚úÖ Connexion r√©ussie √† Hugging Face !")
        except Exception as e:
            logger.error(f"‚ùå Erreur d'authentification Hugging Face: {e}")
            raise ValueError(f"Authentification Hugging Face √©chou√©e: {e}")
        
        # Initialisation du pipeline (lazy loading)
        self._initialize_pipeline()
    
    def _select_optimal_model(self, preferred_model: str) -> str:
        """
        S√©lectionne le mod√®le optimal selon les ressources disponibles.
        
        Args:
            preferred_model: Mod√®le pr√©f√©r√© ou "auto" pour s√©lection automatique
            
        Returns:
            str: Cl√© du mod√®le s√©lectionn√©
        """
        if preferred_model != "auto" and preferred_model in self.MODELS_CONFIG:
            logger.info(f"üéØ Utilisation du mod√®le sp√©cifi√©: {preferred_model}")
            return preferred_model
        
        # S√©lection automatique bas√©e sur les ressources
        available_vram = self._get_available_vram()
        logger.info(f"üíæ VRAM disponible estim√©e: {available_vram:.1f} GB")
        
        # Ordre de pr√©f√©rence: SDXL > SD2.1 > SD1.5
        for model_key in ["sdxl", "sd21", "sd15"]:
            model_config = self.MODELS_CONFIG[model_key]
            if available_vram >= model_config["min_vram_gb"]:
                logger.info(f"‚úÖ Mod√®le {model_key} compatible avec {available_vram:.1f} GB VRAM")
                return model_key
        
        # Fallback sur SD1.5 si aucun mod√®le n'est compatible
        logger.warning("‚ö†Ô∏è VRAM insuffisante, utilisation de SD1.5 par d√©faut")
        return "sd15"
    
    def _get_available_vram(self) -> float:
        """
        Estime la VRAM disponible.
        
        Returns:
            float: VRAM disponible en GB
        """
        if not torch.cuda.is_available():
            return 0.0
        
        try:
            # Obtenir la VRAM totale et utilis√©e
            total_memory = torch.cuda.get_device_properties(0).total_memory
            allocated_memory = torch.cuda.memory_allocated(0)
            available_memory = total_memory - allocated_memory
            
            # Conversion en GB avec marge de s√©curit√© (80% de la VRAM disponible)
            available_gb = (available_memory / (1024**3)) * 0.8
            
            return max(0.0, available_gb)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible d'estimer la VRAM: {e}")
            # Estimation conservative pour CPU ou erreur
            return 4.0 if self.device == "cuda" else 0.0
    
    def _initialize_pipeline(self):
        """
        Initialise le pipeline Stable Diffusion selon le mod√®le s√©lectionn√©.
        
        Utilise le lazy loading pour √©viter de charger le mod√®le si non n√©cessaire.
        Configure automatiquement le device et les optimisations selon le mod√®le.
        """
        try:
            model_id = self.model_config["id"]
            pipeline_class = self.model_config["pipeline_class"]
            
            logger.info(f"üîÑ Initialisation du pipeline {self.selected_model} ({model_id}) sur {self.device}...")
            
            # Configuration optimis√©e selon le device et le mod√®le
            if self.device == "cuda":
                # Configuration CUDA avec optimisations
                self.pipe = pipeline_class.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16,
                    use_safetensors=True,
                    variant="fp16" if self.selected_model == "sdxl" else None
                ).to(self.device)
                
                # Optimisations sp√©cifiques selon le mod√®le
                if self.selected_model == "sdxl":
                    # SDXL n√©cessite plus de VRAM, optimisations agressives
                    self.pipe.enable_model_cpu_offload()
                    self.pipe.enable_attention_slicing()
                    # Scheduler optimis√© pour SDXL
                    self.pipe.scheduler = EulerDiscreteScheduler.from_config(
                        self.pipe.scheduler.config
                    )
                else:
                    # SD 2.1 et 1.5 - optimisations standard
                    self.pipe.enable_attention_slicing()
                    if self.selected_model == "sd21":
                        self.pipe.enable_model_cpu_offload()
                
            else:
                logger.warning("‚ö†Ô∏è CUDA non disponible, utilisation du CPU (plus lent)")
                self.pipe = pipeline_class.from_pretrained(
                    model_id,
                    torch_dtype=torch.float32
                ).to(self.device)
            
            self.current_model = self.selected_model
            logger.info(f"‚úÖ Pipeline {self.selected_model} initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation du pipeline {self.selected_model}: {e}")
            # Fallback vers SD1.5 en cas d'erreur
            if self.selected_model != "sd15":
                logger.info("üîÑ Tentative de fallback vers SD1.5...")
                self._fallback_to_sd15()
            else:
                raise RuntimeError(f"Impossible d'initialiser Stable Diffusion: {e}")
    
    def _fallback_to_sd15(self):
        """
        Fallback vers SD1.5 en cas d'√©chec du mod√®le principal.
        """
        try:
            self.selected_model = "sd15"
            self.model_config = self.MODELS_CONFIG["sd15"]
            
            logger.info("üîÑ Initialisation du fallback SD1.5...")
            
            if self.device == "cuda":
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    self.model_config["id"],
                    torch_dtype=torch.float16,
                    use_safetensors=True
                ).to(self.device)
                self.pipe.enable_attention_slicing()
            else:
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    self.model_config["id"],
                    torch_dtype=torch.float32
                ).to(self.device)
            
            self.current_model = "sd15"
            logger.info("‚úÖ Fallback SD1.5 initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå √âchec du fallback SD1.5: {e}")
            raise RuntimeError(f"Impossible d'initialiser le fallback SD1.5: {e}")
    
    def _build_cocktail_prompt(self, cocktail_data: Dict[str, Any]) -> str:
        """
        Construit un prompt optimis√© pour la g√©n√©ration d'image de cocktail.
        
        Args:
            cocktail_data: Donn√©es du cocktail contenant nom, ingr√©dients, description
        
        Returns:
            str: Prompt optimis√© pour Stable Diffusion
        """
        cocktail_name = cocktail_data.get('name', 'Cocktail')
        ingredients = cocktail_data.get('ingredients', [])
        description = cocktail_data.get('description', '')
        image_prompt = cocktail_data.get('image_prompt', '')
        
        # Construction d'un prompt court et optimis√© (ignorer l'image_prompt long)
        color_hints = self._extract_color_hints(ingredients, description)
        style_hints = self._extract_style_hints(cocktail_name, description)
        
        # Prompt de base tr√®s concis
        base_prompt = f"Beautiful {cocktail_name} cocktail, elegant glass, {color_hints}, {style_hints}"
        
        # Limitation stricte √† 40 mots pour √©viter compl√®tement la troncature CLIP
        words = base_prompt.split()
        if len(words) > 35:  # Garde de la place pour les qualificatifs finaux
            base_prompt = ' '.join(words[:35])
        
        # Ajout de qualificatifs tr√®s courts
        enhanced_prompt = f"{base_prompt}, high quality, professional"
        
        # V√©rification finale - maximum 40 mots
        final_words = enhanced_prompt.split()
        if len(final_words) > 40:
            enhanced_prompt = ' '.join(final_words[:40])
        
        logger.info(f"üé® Prompt g√©n√©r√© ({len(enhanced_prompt.split())} mots): {enhanced_prompt[:100]}...")
        return enhanced_prompt
    
    def _extract_color_hints(self, ingredients: list, description: str) -> str:
        """
        Extrait des indices de couleur bas√©s sur les ingr√©dients et la description.
        
        Args:
            ingredients: Liste des ingr√©dients du cocktail
            description: Description textuelle du cocktail
        
        Returns:
            str: Indices de couleur pour le prompt
        """
        color_map = {
            'rhum': 'golden amber',
            'whisky': 'deep amber',
            'vodka': 'crystal clear',
            'gin': 'clear with botanical hints',
            'tequila': 'pale gold',
            'grenadine': 'ruby red gradient',
            'blue': 'vibrant blue',
            'rouge': 'deep red',
            'vert': 'emerald green',
            'citron': 'bright yellow',
            'orange': 'sunset orange',
            'cranberry': 'deep crimson',
            'passion': 'tropical orange'
        }
        
        colors = []
        text_to_analyze = ' '.join(ingredients).lower() + ' ' + description.lower()
        
        for ingredient, color in color_map.items():
            if ingredient in text_to_analyze:
                colors.append(color)
        
        if colors:
            return f"with {', '.join(colors[:2])} tones"
        return "with elegant color palette"
    
    def _extract_style_hints(self, name: str, description: str) -> str:
        """
        Extrait des indices de style bas√©s sur le nom et la description.
        
        Args:
            name: Nom du cocktail
            description: Description du cocktail
        
        Returns:
            str: Indices de style pour le prompt
        """
        style_keywords = {
            'tropical': 'tropical paradise setting with palm leaves',
            'summer': 'bright summer atmosphere',
            'winter': 'cozy winter ambiance',
            'elegant': 'sophisticated upscale bar',
            'vintage': 'classic vintage style',
            'modern': 'contemporary minimalist design',
            'exotic': 'exotic luxurious presentation',
            'classic': 'timeless classic cocktail style'
        }
        
        text_to_analyze = (name + ' ' + description).lower()
        
        for keyword, style in style_keywords.items():
            if keyword in text_to_analyze:
                return style
        
        return "elegant bar atmosphere"
    
    def _get_negative_prompt(self) -> str:
        """
        Retourne le prompt n√©gatif optimis√© selon le mod√®le utilis√©.
        
        Returns:
            str: Prompt n√©gatif adapt√© au mod√®le
        """
        base_negative = "blurry, low quality, distorted, ugly, bad anatomy"
        
        if self.current_model == "sdxl":
            # SDXL b√©n√©ficie de prompts n√©gatifs plus d√©taill√©s
            return f"{base_negative}, worst quality, low resolution, jpeg artifacts, watermark, signature, text, logo"
        elif self.current_model == "sd21":
            # SD 2.1 avec prompts n√©gatifs mod√©r√©s
            return f"{base_negative}, worst quality, low resolution, jpeg artifacts"
        else:
            # SD 1.5 avec prompts n√©gatifs simples
            return base_negative
    
    def _sanitize_filename(self, cocktail_name: str) -> str:
        """
        Nettoie le nom du cocktail pour cr√©er un nom de fichier valide.
        
        Args:
            cocktail_name: Nom original du cocktail
        
        Returns:
            str: Nom de fichier s√©curis√©
        """
        # Remplacer les caract√®res sp√©ciaux et espaces
        clean_name = re.sub(r'[^a-zA-Z0-9\s]', '', cocktail_name)
        clean_name = re.sub(r'\s+', '_', clean_name.strip())
        clean_name = clean_name.lower()
        
        # Limiter la longueur
        if len(clean_name) > 30:
            clean_name = clean_name[:30]
        
        return clean_name or 'cocktail'
    
    def generate_cocktail_image(self, cocktail_data: Dict[str, Any]) -> Optional[str]:
        """
        G√©n√®re une image de cocktail en utilisant le mod√®le Stable Diffusion s√©lectionn√©.
        
        Args:
            cocktail_data: Dictionnaire contenant les donn√©es du cocktail
                          (name, ingredients, description, image_prompt)
        
        Returns:
            Optional[str]: Chemin relatif vers l'image g√©n√©r√©e, ou None en cas d'erreur
        """
        try:
            # V√©rification de l'√©tat du pipeline
            if not self.pipe:
                logger.error("‚ùå Pipeline Stable Diffusion non initialis√©")
                return None
                
            # V√©rification que le pipeline est sur le bon device
            if hasattr(self.pipe, 'device') and self.pipe.device != self.device:
                logger.warning(f"‚ö†Ô∏è Pipeline sur device {self.pipe.device}, attendu {self.device}")
            
            cocktail_name = cocktail_data.get('name', 'Cocktail Inconnu')
            logger.info(f"üé® G√©n√©ration d'image pour le cocktail: {cocktail_name}")
            logger.info(f"üìä Mod√®le utilis√©: {self.current_model} - {self.model_config['description']}")
            
            # Construction du prompt optimis√©
            prompt = self._build_cocktail_prompt(cocktail_data)
            
            # G√©n√©ration de l'image
            logger.info(f"üîÑ G√©n√©ration en cours avec {self.current_model}...")
            
            # Param√®tres optimis√©s selon le mod√®le
            width, height = self.model_config["resolution"]
            generation_params = {
                "prompt": prompt,
                "num_inference_steps": self.model_config["steps"],
                "guidance_scale": self.model_config["guidance_scale"],
                "width": width,
                "height": height,
                "negative_prompt": self._get_negative_prompt(),
                "num_images_per_prompt": 1
            }
            
            # Param√®tres sp√©cifiques pour SDXL
            if self.current_model == "sdxl":
                generation_params.update({
                    "denoising_end": 0.8,  # Utilise le refiner implicitement
                    "output_type": "pil"
                })
            
            logger.info(f"‚öôÔ∏è Param√®tres: {width}x{height}, {self.model_config['steps']} √©tapes, guidance {self.model_config['guidance_scale']}")
            
            # G√©n√©ration avec gestion d'erreur robuste
            result = self.pipe(**generation_params)
            
            # V√©rification que le r√©sultat contient des images
            if not hasattr(result, 'images') or not result.images:
                logger.error("‚ùå Aucune image g√©n√©r√©e par le pipeline")
                return None
                
            image = result.images[0]
            
            # V√©rification que l'image est valide
            if image is None:
                logger.error("‚ùå Image g√©n√©r√©e est None")
                return None
            
            # Sauvegarde de l'image
            filename = self._save_image(image, cocktail_name)
            if filename:
                logger.info(f"‚úÖ Image g√©n√©r√©e et sauvegard√©e: {filename}")
                return filename
            else:
                logger.error("‚ùå Erreur lors de la sauvegarde de l'image")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration d'image: {e}")
            return None
    
    def _save_image(self, image: Image.Image, cocktail_name: str) -> Optional[str]:
        """
        Sauvegarde l'image g√©n√©r√©e dans le dossier public du frontend.
        
        Args:
            image: Image PIL g√©n√©r√©e
            cocktail_name: Nom du cocktail pour le nom de fichier
        
        Returns:
            Optional[str]: Chemin relatif vers l'image sauvegard√©e
        """
        try:
            # Cr√©ation du nom de fichier unique
            clean_name = self._sanitize_filename(cocktail_name)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cocktail_{clean_name}_{timestamp}.png"
            
            # Chemin vers le dossier public du frontend
            frontend_public_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                'frontend', 'public'
            )
            
            # Cr√©er le dossier s'il n'existe pas
            os.makedirs(frontend_public_dir, exist_ok=True)
            
            # Chemin complet du fichier
            file_path = os.path.join(frontend_public_dir, filename)
            
            # Sauvegarde avec optimisation
            image.save(file_path, "PNG", optimize=True, quality=95)
            
            # Retourner le chemin relatif pour l'URL
            relative_path = f"/{filename}"
            logger.info(f"üíæ Image sauvegard√©e: {file_path}")
            
            return relative_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde: {e}")
            return None
    
    def test_connection(self) -> bool:
        """
        Teste la connexion et la fonctionnalit√© du service Stable Diffusion.
        
        Returns:
            bool: True si le service fonctionne correctement
        """
        try:
            logger.info("üß™ Test de connexion Stable Diffusion...")
            
            if not self.pipe:
                logger.error("‚ùå Pipeline non initialis√©")
                return False
            
            # Test avec un prompt simple
            test_prompt = "A simple elegant cocktail glass, professional photography"
            
            # G√©n√©ration de test avec param√®tres r√©duits pour la rapidit√©
            result = self.pipe(
                prompt=test_prompt,
                num_inference_steps=10,  # R√©duit pour le test
                guidance_scale=7.5,
                width=512,               # R√©solution r√©duite pour le test
                height=512
            )
            
            if result and result.images and len(result.images) > 0:
                logger.info("‚úÖ Test de g√©n√©ration Stable Diffusion r√©ussi")
                return True
            else:
                logger.error("‚ùå Test de g√©n√©ration √©chou√©: aucune image g√©n√©r√©e")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Test de connexion √©chou√©: {e}")
            return False
    
    def get_service_info(self) -> Dict[str, Any]:
        """
        Retourne les informations sur le service de g√©n√©ration d'images.
        
        Returns:
            Dict[str, Any]: Informations sur le service
        """
        return {
            "service_name": "Stable Diffusion 3.5 Large",
            "model_id": self.model_id,
            "device": self.device,
            "cuda_available": torch.cuda.is_available(),
            "pipeline_loaded": self.pipe is not None,
            "hf_authenticated": bool(self.hf_token and self.hf_token != '<VOTRE-TOKEN-HUGGINGFACE>')
        }