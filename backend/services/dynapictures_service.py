#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Service de gÃ©nÃ©ration d'images avec Stable Diffusion (DynaPictures)
Version simplifiÃ©e utilisant les prompts gÃ©nÃ©rÃ©s par Mistral
"""

import os
import torch
import logging
from datetime import datetime
from typing import Optional
from diffusers import DiffusionPipeline, DDIMScheduler
import PIL.Image

logger = logging.getLogger(__name__)

class DynaPicturesService:
    """
    Service simplifiÃ© de gÃ©nÃ©ration d'images avec Stable Diffusion.
    Utilise les prompts fournis par Mistral pour gÃ©nÃ©rer des images de cocktails.
    """
    
    def __init__(self):
        """
        Initialise le service avec le pipeline Stable Diffusion.
        """
        self.pipeline = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.output_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'frontend', 'public')
        
        os.makedirs(self.output_dir, exist_ok=True)
        self._load_pipeline()

    def _load_pipeline(self):
        """
        Charge le pipeline Stable Diffusion.
        """
        try:
            model_id = os.getenv('DYNA_PICTURES_MODEL', 'runwayml/stable-diffusion-v1-5')
            logger.info(f"ðŸš€ Chargement du modÃ¨le: {model_id} sur {self.device}")
            
            self.pipeline = DiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            ).to(self.device)
            
            # Remplacer le scheduler par dÃ©faut par DDIMScheduler pour Ã©viter les erreurs
            # Le scheduler PNDM et DPMSolver causent des erreurs d'index
            try:
                self.pipeline.scheduler = DDIMScheduler.from_config(
                    self.pipeline.scheduler.config
                )
                logger.info("âœ… Scheduler remplacÃ© par DDIMScheduler")
            except Exception as scheduler_e:
                logger.warning(f"âš ï¸ Impossible de changer le scheduler: {scheduler_e}")
            
            # Optimisations mÃ©moire (seulement si disponibles)
            try:
                if hasattr(self.pipeline, 'enable_attention_slicing'):
                    self.pipeline.enable_attention_slicing()
                if self.device == "cuda" and hasattr(self.pipeline, 'enable_model_cpu_offload'):
                    self.pipeline.enable_model_cpu_offload()
            except Exception as opt_e:
                logger.warning(f"âš ï¸ Optimisations mÃ©moire non disponibles: {opt_e}")
            
            logger.info("âœ… Pipeline chargÃ© avec succÃ¨s")
            
        except Exception as e:
            logger.error(f"âŒ Erreur chargement pipeline: {e}")
            self.pipeline = None

    def generate_image(self, prompt: str) -> Optional[str]:
        """
        GÃ©nÃ¨re une image Ã  partir du prompt fourni par Mistral.
        
        Args:
            prompt (str): Le prompt d'image gÃ©nÃ©rÃ© par Mistral
        
        Returns:
            Optional[str]: Le chemin relatif de l'image gÃ©nÃ©rÃ©e ou None en cas d'Ã©chec
        """
        if not self.pipeline:
            logger.error("âŒ Pipeline non initialisÃ©")
            return None

        try:
            # Tronquer le prompt si nÃ©cessaire pour Ã©viter les erreurs CLIP (max 77 tokens)
            # Le tokenizer de CLIP est interne au pipeline, nous allons donc tronquer le texte brut
            # Une estimation simple est de limiter Ã  environ 500 caractÃ¨res pour 77 tokens
            # C'est une solution temporaire, une meilleure approche serait d'utiliser le tokenizer rÃ©el
            max_prompt_length = 500 # Approximation pour 77 tokens
            
            logger.info(f"Prompt original: {prompt}")
            processed_prompt = prompt[:max_prompt_length] if len(prompt) > max_prompt_length else prompt

            logger.info(f"ðŸŽ¨ GÃ©nÃ©ration avec prompt Mistral (tronquÃ©): {processed_prompt[:100]}...")
            
            try:
                # ExÃ©cuter le pipeline de gÃ©nÃ©ration d'image
                # Ajout de num_inference_steps pour un contrÃ´le plus fin, valeur par dÃ©faut 50
                # Ajout de guidance_scale pour contrÃ´ler la force du prompt, valeur par dÃ©faut 7.5
                generation_output = self.pipeline(
                    prompt=processed_prompt,
                    negative_prompt="blurry, low quality, distorted, ugly, bad anatomy, text, watermark",
                    num_inference_steps=25, # OptimisÃ© pour DDIMScheduler
                    guidance_scale=7.5, # Force du prompt
                    height=512, # Taille fixe pour Ã©viter les problÃ¨mes de dimensions
                    width=512
                )
            except Exception as pipeline_e:
                logger.error(f"âŒ Erreur lors de l'exÃ©cution du pipeline de gÃ©nÃ©ration: {pipeline_e}", exc_info=True)
                return None
            
            # VÃ©rifier si la gÃ©nÃ©ration a produit une image valide
            logger.debug(f"Type de generation_output: {type(generation_output)}")
            logger.debug(f"Type de generation_output.images: {type(generation_output.images)}")
            if generation_output.images:
                logger.debug(f"Type de generation_output.images[0]: {type(generation_output.images[0])}")

            if generation_output.images and isinstance(generation_output.images[0], PIL.Image.Image):
                image = generation_output.images[0]
            else:
                logger.error("âŒ La gÃ©nÃ©ration d'image a Ã©chouÃ©: aucune image valide (PIL.Image.Image) retournÃ©e.")
                return None
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cocktail_{timestamp}.png"
            filepath = os.path.join(self.output_dir, filename)
            
            image.save(filepath)
            logger.info(f"âœ… Image sauvegardÃ©e: {filename}")
            
            return f"/cocktail_{timestamp}.png"

        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration: {e}")
            return None

    def generate_cocktail_image(self, cocktail_data: dict) -> Optional[str]:
        """
        GÃ©nÃ¨re une image pour un cocktail en utilisant le prompt fourni par Mistral.
        
        Args:
            cocktail_data (dict): DonnÃ©es du cocktail avec 'image_prompt' de Mistral
        
        Returns:
            Optional[str]: Le chemin relatif de l'image gÃ©nÃ©rÃ©e ou None en cas d'Ã©chec
        """
        if not cocktail_data:
            logger.error("âŒ DonnÃ©es cocktail manquantes")
            return None
        
        # Utilise le prompt gÃ©nÃ©rÃ© par Mistral
        prompt = cocktail_data.get('image_prompt')
        if not prompt:
            logger.error("âŒ Prompt d'image Mistral manquant")
            return None
        
        cocktail_name = cocktail_data.get('name', 'Cocktail')
        logger.info(f"ðŸŽ¨ GÃ©nÃ©ration image pour: {cocktail_name}")
        
        return self.generate_image(prompt)
    


    def is_available(self) -> bool:
        """
        VÃ©rifie si le service est disponible.
        """
        return self.pipeline is not None